{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4518fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting job extraction...\n",
      "Processed page 1 - Found 10 jobs\n",
      "No more results found on page 2\n",
      "\n",
      "Saved 10 jobs to software_engineer_jobs.json\n",
      "\n",
      "Extraction completed successfully!\n",
      "Total jobs extracted: 10\n",
      "\n",
      "Sample of first job entry:\n",
      "title: Lead Software Engineer-Java, Bank Modernization\n",
      "company: Capital One\n",
      "location: New York, NY\n",
      "description: 114 5th Ave (22114), United States of America, New York, New York\n",
      "\n",
      "Lead Software Engineer-Java, Bank Modernization\n",
      "\n",
      "Do you love building and pioneerin...\n",
      "via: Capital One Careers\n",
      "posted_at: 5 days ago\n",
      "schedule_type: Full-time and Part-time\n",
      "salary: N/A\n",
      "benefits: ['Health insurance']\n",
      "apply_link: https://www.capitalonecareers.com/job/new-york/lead-software-engineer-java-bank-modernization/1732/73175694832?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic\n"
     ]
    }
   ],
   "source": [
    "#saves only 1 page with all params in json resp even when requested for 3 pages\n",
    "from serpapi import GoogleSearch\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "def extract_jobs_data(api_key, num_pages=3):\n",
    "    all_jobs = []\n",
    "    \n",
    "    for page in range(num_pages):\n",
    "        # Parameters for the API request\n",
    "        params = {\n",
    "            'api_key': api_key,                           # Your API key\n",
    "            'engine': 'google_jobs',                      # Search engine\n",
    "            'q': 'software engineer',                     # Search query\n",
    "            'hl': 'en',                                  # Language\n",
    "            'gl': 'us',                                  # Country\n",
    "            'google_domain': 'google.com',               # Google domain\n",
    "            'start': page * 10 if page > 0 else None,    # Pagination\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Make the API request\n",
    "            search = GoogleSearch(params)\n",
    "            results = search.get_dict()\n",
    "            \n",
    "            # Check if we have job results and it's not empty\n",
    "            if 'jobs_results' not in results or not results['jobs_results']:\n",
    "                print(f\"No more results found on page {page + 1}\")\n",
    "                break\n",
    "            \n",
    "            # Extract specific parameters from each job\n",
    "            for job in results['jobs_results']:\n",
    "                job_data = {\n",
    "                    'title': job.get('title', 'N/A'),\n",
    "                    'company': job.get('company_name', 'N/A'),\n",
    "                    'location': job.get('location', 'N/A'),\n",
    "                    'description': job.get('description', 'N/A')[:500] + '...' if job.get('description') else 'N/A',\n",
    "                    'via': job.get('via', 'N/A'),\n",
    "                    'posted_at': job.get('detected_extensions', {}).get('posted_at', 'N/A'),\n",
    "                    'schedule_type': job.get('detected_extensions', {}).get('schedule_type', 'N/A'),\n",
    "                    'salary': job.get('detected_extensions', {}).get('salary', 'N/A'),\n",
    "                    'benefits': [ext for ext in job.get('extensions', []) if 'insurance' in ext.lower() or 'benefit' in ext.lower()],\n",
    "                    'apply_link': job.get('apply_options', [{}])[0].get('link', 'N/A') if job.get('apply_options') else 'N/A'\n",
    "                }\n",
    "                all_jobs.append(job_data)\n",
    "                \n",
    "            print(f\"Processed page {page + 1} - Found {len(results['jobs_results'])} jobs\")\n",
    "            \n",
    "            # Get next page token if available\n",
    "            if 'serpapi_pagination' in results and 'next_page_token' in results['serpapi_pagination']:\n",
    "                params['next_page_token'] = results['serpapi_pagination']['next_page_token']\n",
    "            else:\n",
    "                break  # No more pages available\n",
    "                \n",
    "            # Add a small delay between requests\n",
    "            time.sleep(2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing page {page + 1}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return all_jobs\n",
    "\n",
    "def save_to_json(jobs_data, filename='software_engineer_jobs.json'):\n",
    "    \"\"\"Save the extracted jobs data to a JSON file\"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(jobs_data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"\\nSaved {len(jobs_data)} jobs to {filename}\")\n",
    "\n",
    "def main():\n",
    "    # Your SerpApi key\n",
    "    API_KEY = \"your key\"\n",
    "    \n",
    "    try:\n",
    "        # Extract jobs data\n",
    "        print(\"Starting job extraction...\")\n",
    "        jobs_data = extract_jobs_data(API_KEY)\n",
    "        \n",
    "        if not jobs_data:\n",
    "            print(\"No jobs were found!\")\n",
    "            return\n",
    "            \n",
    "        # Save to JSON file\n",
    "        save_to_json(jobs_data)\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\nExtraction completed successfully!\")\n",
    "        print(f\"Total jobs extracted: {len(jobs_data)}\")\n",
    "        \n",
    "        # Print sample of first job\n",
    "        if jobs_data:\n",
    "            print(\"\\nSample of first job entry:\")\n",
    "            first_job = jobs_data[0]\n",
    "            for key, value in first_job.items():\n",
    "                if key == 'description':\n",
    "                    print(f\"{key}: {value[:150]}...\")  # Show first 150 chars of description\n",
    "                else:\n",
    "                    print(f\"{key}: {value}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843164cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting job extraction...\n",
      "\n",
      "Checking response structure for page 1:\n",
      "Pagination info found\n",
      "Next page token: eyJmYyI6IkVvd0ZDc3dFUVVwSE9VcHJVRUZ5TUhVMGNtWTBVM28wVlZCMmJGSjFOVkptZVZRMWRWWjRNRFU0TmpsU2FrOWpibkpWUVVwVWFYWkVaVXR0U1ZkMVdETmlSbGhwZFcxSU9GQkpabWhTUzBOQ2VHRm5ielpYV2xCaFJ6YzFhbU5xZG1SV2FHRlphMVIxYW1KUVNtUlJiM3BvWkZORWJIcDBOVU42YjA4M05IUXdOWFY2ZVdkaVNsOXpSbEZqVG1Gck1sRndRbmh5VVVORmJsRlpZazF1UWtsSFpURmZTWEpFWTBWcWVGUlBRaTFUTVU4dE9UaDBkMWgxZFU4ek5tTlpVeko1V0ZwdU5rOW5ja1ZEY0ROVk4wRlJXbUYwUVVJeVJreGFlVlZSU25sbFMzWTJNMlJQYW1GeGFHUlhMWEZsU2xoV1NXcHdWRTVLUW5wbmR6Qm9SMkZCUjNKSmVtOVBiMEpUWTBWNFZVZExTVk5ZWVVaQ1pISllkVzh4WmpKcVRubzJXRzVyU1RObGJWOUJaVmhRU1RKU1dVSXhXbUp1Y3pOU1NtSldjRU13U2pVeGVGTmljV1Z1UVdFeFQxWnJlWGhhVVRCR01VdGxOMFZOY0RNMGVYVnVVbU54UVU5elRrazVXVTk1TUhvMWNFZzNTbEJKU1ZOT1RqaFJlbU01ZWt0UlowZDJPREpQVjJGU1MwUlFTaTFoVWt0SVpVOXlUakZqUjJWT2JYWjZNbk0xY0dSdFZVVldibEJYZFdoM1oyUlFPVU5XYUZCWGVIRkJaamx2YVVWeVQzUTNaMU4wTTFSRVpFbHFiblY2YURGblNFcFJRMDR0Vm5ORGRXUk5jbkJDZUdOa2VuaFdhMHBrWmtac2JURlVSbGM0Um1wWVRXVjJZWFZyY0hWRWJXbGxiWGRCWDJaRlNGVXRkR1pvTjNFNFQwSTBjRzB3ZGs5V2NUbG9TSFJPVUhSbVUyWnBWVWxqU2kxSVJHdHFRV2QwUzBNeU5uRk9hV1Z1VWxWUlRXa3RjSGxwWjFwamJGTTFUek5zTkhkQkVoZHpTMEZmV2psUFowUm1jV2cxVG05UWJIUlVTbTFCYnhvaVFVWllja1ZqYjJwa2VXdG5XaTFFTlVSa1ZXWjVkVVowVkU5eU1XMDNWa2RTWnciLCJmY3YiOiIzIn0=\n",
      "Found 10 jobs on page 1\n",
      "Successfully processed page 1 - Total jobs so far: 10\n",
      "\n",
      "Checking response structure for page 2:\n",
      "Pagination info found\n",
      "Next page token: eyJmYyI6IkVvd0ZDc3dFUVVwSE9VcHJUMUY1TmpSNVJXSlZjbTEwY21ac09XNWZZM1ZNZFRRM1ZVRXpZWEZ0ZVhndFExSnVNbXAyVHpKS1JtdDNTREoyY1c5SU1HcG5Sbk5TWkhsNVRuWkhNMHR1Umt4aGJFZERkVk5DWlRObmJHVnNXRkZMWlRsTkxVOXdUMVJuTm05UlNuTllSMUo0VG01VWJFVnNiR2g0YlY5dGRESllMVEZzUjBWQ1NGRm9Ta2R4T0hBMmFuVjVhM2szYURsQlpGUldZekJ2U0VkTWJrOXZZelpSZGtSYU5XbHJlbFpwTlRBeWREZFRXVE14ZGpSSU5HMUpkRzlNTW1NMVpqWjNSMFI2TVhSRlVFVjJSWFZVTFhkMGFXTk1Ra2hKUVRjelEwcE1jRk5JVW5jMk5WaFViSGhCVkVSVVJsSlZVR3RSWkRrM2RXNURaWEV5ZEVSYVREUm1WbWxWVFhkR1NtcFNNazB0YjBsa01rVkxhRlU1YlZNMVEybFVTbGRSWTFWcmVrODVSVmRZUVVkYVRVWm5lbGxVZFRWZmRuWlFMVTlpVVZGNFJuUjNSVFJMZDA1TlFqSlVZWE5aY21kWWFuVmtlVEV6WW01cWJXbzVVakJ5UVc5SlZHeDFjek0xTWtWNGF6UkJjbXBYYUROUldWSjNOV2RITjBSWE1qbE1TMll3WHkxU2FHVnhka0ZEVlhoSlJYZGhRVEkzUkdoVU9USmZjalpZVEZKd2VISlZMVWhyVTJsWmJ6bHlkM2g2Y3kxa1pHMXBRamxLTTNWRk1VaDBXbmh6UkRCRWQwbFNSblkwVld4eGFYUTRlV0ZaT1ZOaFgyNTBjRlJhWm5SRldrbHdXbXR4TjJWV1IyUTNjVzV5U1haaGMxaE5TVzg1WWtKaFYxZzJiazgxVm5kcVZXZE1aaTExUVRWQ1pVZHhaMEkzYlhKbWMydGxNekpzY1hadFVrWXhPVWRYVW5SMlJHVk5hM2RRUms1TWNraFdkVE5QVkdWYWEwaEliM0pZWWprdFgwZDRMV05sU1VwdUVoZHpOa0ZmV2pZMmNFdHhXRXByVUVsUWNUWlFRamhCVVJvaVFVWllja1ZqYnpkM2FHSkdaV3MxUW10dGVVOVhXRll4TlRBNFZTMTFVazVRZHciLCJmY3YiOiIzIn0=\n",
      "Found 10 jobs on page 2\n",
      "Successfully processed page 2 - Total jobs so far: 20\n",
      "\n",
      "Checking response structure for page 3:\n",
      "Pagination info found\n",
      "Next page token: eyJmYyI6IkV1SUVDcUlFUVVwSE9VcHJVSFJrWlRnM2JUbGFNRmRzTUVkNFEwNVlRamxLT1daaVFVUnlWMjVWTFVkSVRGOWpSREo1YjNjeVZHSkJibEJGVmt4NVptUkpRMWRHVDNVNGJuQTVRbkJmWjFseUxVZEdSSFpuTURNM2VUVTVlRVJIVERsYVZFSkdMWFJKYkdKZk1WbGFPRlZqWW5SUk9VNTNhM040TlZrM1pEVXlZV2t3VmtZMGJUQXRaWEJRUW1VMGJtNXlSR3RMVURORmNWbFBaalJoTjBwTlN6WkdNVGx4VVhZMlZHTTFVM1kwUW5jMmQxZHZiMDFPWVRWV1lrZE9jamN6TkhOR1JGcEhZbTFsZW5GU1JuRlpYMWRSYmw5U1JYaDVUazVWYjE5SWNVOXplRk50UWt4Wk9IbGtPVFZzV0hoQ2NFVklNRXA1VUZKVFFrbFNTRGRUUWpkMFlVbEhTM1J1U2xSMExWVmxWMWMxZVdzME4wcEVhbkZIWlZWYVZWWjVNV2hvZVRoZlFtaE9lRlUzYmkxSGFHSklUMFJsVmtockxVdEVRa2xsTmpNMGVUbEhjVFpxZVhGdlgyaFpiVTFmWkcxRVpsRjVabUpFZDBGUVEzUm5PVzh6UlRFNFlsbHFOazlEUkRRd0xYaGhPRWsxTjJ4MVMyVlNlRXhYUm5oSGVXZDFWa1pIVjFKWFNFaHZUMFZUWlhCV1kzZGxaa05rYTI5SlMybFFVelJyU2xJMGJtaEdkMmhZUnpWeE9YRnhNV0pZUVhsaU4wNHRVMHBZVDA1WFJIUkVSbmd5YzFaTFIwNDJVbnBWYTBkTFdUUnVhSEF4V1VkVFNGcDFkbXA0Y1hSRllVcDVkVWxGVDJGUVN6SmZhRkF4TUhCS1JFMWZaM2xXUkV4U1RHVmljRkJGVFU1cGRrb3piRkpJTVVSNlVqRmhNRkJVWlVOV1pETkNaM0psYzJGeFdXbG5FaGQwTmtGZldqUTJaazV4VkZCM1ltdFFNR1ZoYkhGUlp4b2lRVVpZY2tWamNqTjBXRTVhVkdoaVpFWnNkV2xGUVdKT1VsbzBOMVJ5Ym5WaGR3IiwiZmN2IjoiMyJ9\n",
      "Found 7 jobs on page 3\n",
      "Successfully processed page 3 - Total jobs so far: 27\n",
      "\n",
      "Saved 27 jobs to software_engineer_jobs.json\n",
      "\n",
      "Extraction completed successfully!\n",
      "Total jobs extracted: 27\n",
      "\n",
      "First job entry:\n",
      "title: Software Engineer Early Career\n",
      "company: Lockheed Martin\n",
      "location: Littleton, CO\n",
      "description: Description:By bringing together people that use their passion for purposeful innovation, at Lockheed Martin we keep people safe and solve the world's...\n",
      "via: Lockheed Martin\n",
      "posted_at: 2 days ago\n",
      "schedule_type: Full-time\n",
      "salary: N/A\n",
      "benefits: ['Dental insurance', 'Health insurance']\n",
      "apply_link: https://www.lockheedmartinjobs.com/job/littleton/software-engineer-early-career/694/73281245104?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic\n",
      "\n",
      "Last job entry:\n",
      "title: Software Engineer\n",
      "company: ManTech\n",
      "location: Philadelphia, PA\n",
      "description: ManTech is seeking a motivated, career and customer-oriented Software Engineer to join our team in Philadelphia, PA. This is an onsite position.\n",
      "\n",
      "Resp...\n",
      "via: Indeed\n",
      "posted_at: 8 days ago\n",
      "schedule_type: Full-time\n",
      "salary: N/A\n",
      "benefits: []\n",
      "apply_link: https://www.indeed.com/viewjob?jk=71f38881cd62bb46&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic\n"
     ]
    }
   ],
   "source": [
    "#saves 3 pages data all params in json resp when requested for 3 pages\n",
    "from serpapi import GoogleSearch\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "def extract_jobs_data(api_key, num_pages=3):\n",
    "    all_jobs = []\n",
    "    \n",
    "    for page in range(num_pages):\n",
    "        # Parameters for the API request\n",
    "        params = {\n",
    "            'api_key': api_key,                           # Your API key\n",
    "            'engine': 'google_jobs',                      # Search engine\n",
    "            'q': 'software engineer',                     # Search query\n",
    "            'hl': 'en',                                  # Language\n",
    "            'gl': 'us',                                  # Country\n",
    "            'google_domain': 'google.com',               # Google domain\n",
    "            'start': page * 10 if page > 0 else None,    # Pagination\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Make the API request\n",
    "            search = GoogleSearch(params)\n",
    "            results = search.get_dict()\n",
    "            \n",
    "            # Check if we have job results and it's not empty\n",
    "            if 'jobs_results' not in results or not results['jobs_results']:\n",
    "                print(f\"No more results found on page {page + 1}\")\n",
    "                break\n",
    "            \n",
    "            # Extract specific parameters from each job\n",
    "            for job in results['jobs_results']:\n",
    "                job_data = {\n",
    "                    'title': job.get('title', 'N/A'),\n",
    "                    'company': job.get('company_name', 'N/A'),\n",
    "                    'location': job.get('location', 'N/A'),\n",
    "                    'description': job.get('description', 'N/A')[:500] + '...' if job.get('description') else 'N/A',\n",
    "                    'via': job.get('via', 'N/A'),\n",
    "                    'posted_at': job.get('detected_extensions', {}).get('posted_at', 'N/A'),\n",
    "                    'schedule_type': job.get('detected_extensions', {}).get('schedule_type', 'N/A'),\n",
    "                    'salary': job.get('detected_extensions', {}).get('salary', 'N/A'),\n",
    "                    'benefits': [ext for ext in job.get('extensions', []) if 'insurance' in ext.lower() or 'benefit' in ext.lower()],\n",
    "                    'apply_link': job.get('apply_options', [{}])[0].get('link', 'N/A') if job.get('apply_options') else 'N/A'\n",
    "                }\n",
    "                all_jobs.append(job_data)\n",
    "                \n",
    "            print(f\"Processed page {page + 1} - Found {len(results['jobs_results'])} jobs\")\n",
    "            \n",
    "            # Get next page token if available\n",
    "            if 'serpapi_pagination' in results and 'next_page_token' in results['serpapi_pagination']:\n",
    "                params['next_page_token'] = results['serpapi_pagination']['next_page_token']\n",
    "            else:\n",
    "                break  # No more pages available\n",
    "                \n",
    "            # Add a small delay between requests\n",
    "            time.sleep(2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing page {page + 1}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return all_jobs\n",
    "\n",
    "def save_to_json(jobs_data, filename='software_engineer_jobs.json'):\n",
    "    \"\"\"Save the extracted jobs data to a JSON file\"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(jobs_data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"\\nSaved {len(jobs_data)} jobs to {filename}\")\n",
    "\n",
    "def main():\n",
    "    # Your SerpApi key\n",
    "    API_KEY = \"your key\"\n",
    "    \n",
    "    try:\n",
    "        # Extract jobs data\n",
    "        print(\"Starting job extraction...\")\n",
    "        jobs_data = extract_jobs_data(API_KEY)\n",
    "        \n",
    "        if not jobs_data:\n",
    "            print(\"No jobs were found!\")\n",
    "            return\n",
    "            \n",
    "        # Save to JSON file\n",
    "        save_to_json(jobs_data)\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\nExtraction completed successfully!\")\n",
    "        print(f\"Total jobs extracted: {len(jobs_data)}\")\n",
    "        \n",
    "        # Print sample of first job\n",
    "        if jobs_data:\n",
    "            print(\"\\nSample of first job entry:\")\n",
    "            first_job = jobs_data[0]\n",
    "            for key, value in first_job.items():\n",
    "                if key == 'description':\n",
    "                    print(f\"{key}: {value[:150]}...\")  # Show first 150 chars of description\n",
    "                else:\n",
    "                    print(f\"{key}: {value}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b08ff1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting google-search-results\n",
      "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests in /Users/sai_vivek_vangala/anaconda3/lib/python3.11/site-packages (from google-search-results) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sai_vivek_vangala/anaconda3/lib/python3.11/site-packages (from requests->google-search-results) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sai_vivek_vangala/anaconda3/lib/python3.11/site-packages (from requests->google-search-results) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sai_vivek_vangala/anaconda3/lib/python3.11/site-packages (from requests->google-search-results) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sai_vivek_vangala/anaconda3/lib/python3.11/site-packages (from requests->google-search-results) (2023.11.17)\n",
      "Building wheels for collected packages: google-search-results\n",
      "  Building wheel for google-search-results (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32003 sha256=5a87bf0a3bfafbde11a8675bf2e13569f09125c32f364a243ba333dcfd19dd94\n",
      "  Stored in directory: /private/var/folders/n6/v6mbk2mj25xc5yqv9prgz43m0000gp/T/pip-ephem-wheel-cache-46un_ssq/wheels/6e/42/3e/aeb691b02cb7175ec70e2da04b5658d4739d2b41e5f73cd06f\n",
      "Successfully built google-search-results\n",
      "Installing collected packages: google-search-results\n",
      "Successfully installed google-search-results-2.4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73b8088d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting serpapi\n",
      "  Obtaining dependency information for serpapi from https://files.pythonhosted.org/packages/df/6a/21deade04100d64844e494353a5d65e7971fbdfddf78eb1f248423593ad0/serpapi-0.1.5-py2.py3-none-any.whl.metadata\n",
      "  Downloading serpapi-0.1.5-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: requests in /Users/sai_vivek_vangala/anaconda3/lib/python3.11/site-packages (from serpapi) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sai_vivek_vangala/anaconda3/lib/python3.11/site-packages (from requests->serpapi) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sai_vivek_vangala/anaconda3/lib/python3.11/site-packages (from requests->serpapi) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sai_vivek_vangala/anaconda3/lib/python3.11/site-packages (from requests->serpapi) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sai_vivek_vangala/anaconda3/lib/python3.11/site-packages (from requests->serpapi) (2023.11.17)\n",
      "Downloading serpapi-0.1.5-py2.py3-none-any.whl (10 kB)\n",
      "Installing collected packages: serpapi\n",
      "Successfully installed serpapi-0.1.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install serpapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33efe0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saves all 3 pages with all params in json resp\n",
    "from serpapi import GoogleSearch\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "def extract_jobs_data(api_key, num_pages=3):\n",
    "    all_jobs = []\n",
    "    current_page = 1\n",
    "    next_page_token = None\n",
    "    \n",
    "    while current_page <= num_pages:\n",
    "        # Parameters for the API request\n",
    "        params = {\n",
    "            'api_key': api_key,                  \n",
    "            'engine': 'google_jobs',             \n",
    "            'q': 'software engineer',            \n",
    "            'hl': 'en',                         \n",
    "            'gl': 'us',                         \n",
    "            'google_domain': 'google.com'        \n",
    "        }\n",
    "        \n",
    "        # Add next_page_token if we have one (for pages after first)\n",
    "        if next_page_token:\n",
    "            params['next_page_token'] = next_page_token\n",
    "        \n",
    "        try:\n",
    "            # Make the API request\n",
    "            search = GoogleSearch(params)\n",
    "            results = search.get_dict()\n",
    "            \n",
    "            # Debug print to see the structure (optional)\n",
    "            print(f\"\\nChecking response structure for page {current_page}:\")\n",
    "            if 'serpapi_pagination' in results:\n",
    "                print(\"Pagination info found\")\n",
    "                print(\"Next page token:\", results.get('serpapi_pagination', {}).get('next_page_token', 'None'))\n",
    "            \n",
    "            # Check if we have job results and it's not empty\n",
    "            if 'jobs_results' not in results or not results['jobs_results']:\n",
    "                print(f\"No more results found on page {current_page}\")\n",
    "                break\n",
    "            \n",
    "            # Extract jobs from current page\n",
    "            jobs_on_this_page = results['jobs_results']\n",
    "            print(f\"Found {len(jobs_on_this_page)} jobs on page {current_page}\")\n",
    "            \n",
    "            # Extract specific parameters from each job\n",
    "            for job in jobs_on_this_page:\n",
    "                job_data = {\n",
    "                    'title': job.get('title', 'N/A'),\n",
    "                    'company': job.get('company_name', 'N/A'),\n",
    "                    'location': job.get('location', 'N/A'),\n",
    "                    'description': job.get('description', 'N/A')[:500] + '...' if job.get('description') else 'N/A',\n",
    "                    'via': job.get('via', 'N/A'),\n",
    "                    'posted_at': job.get('detected_extensions', {}).get('posted_at', 'N/A'),\n",
    "                    'schedule_type': job.get('detected_extensions', {}).get('schedule_type', 'N/A'),\n",
    "                    'salary': job.get('detected_extensions', {}).get('salary', 'N/A'),\n",
    "                    'benefits': [ext for ext in job.get('extensions', []) if 'insurance' in ext.lower() or 'benefit' in ext.lower()],\n",
    "                    'apply_link': job.get('apply_options', [{}])[0].get('link', 'N/A') if job.get('apply_options') else 'N/A'\n",
    "                }\n",
    "                all_jobs.append(job_data)\n",
    "            \n",
    "            print(f\"Successfully processed page {current_page} - Total jobs so far: {len(all_jobs)}\")\n",
    "            \n",
    "            # Check for next page token\n",
    "            serpapi_pagination = results.get('serpapi_pagination', {})\n",
    "            next_page_token = serpapi_pagination.get('next_page_token')\n",
    "            \n",
    "            if not next_page_token:\n",
    "                print(f\"No next page token found after page {current_page}\")\n",
    "                break\n",
    "                \n",
    "            current_page += 1\n",
    "            \n",
    "            # Add a small delay between requests\n",
    "            time.sleep(2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing page {current_page}: {str(e)}\")\n",
    "            break\n",
    "    \n",
    "    return all_jobs\n",
    "\n",
    "def save_to_json(jobs_data, filename='software_engineer_jobs.json'):\n",
    "    \"\"\"Save the extracted jobs data to a JSON file\"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(jobs_data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"\\nSaved {len(jobs_data)} jobs to {filename}\")\n",
    "\n",
    "def main():\n",
    "    # Your SerpApi key\n",
    "    API_KEY = \"your key\"\n",
    "    \n",
    "    try:\n",
    "        # Extract jobs data\n",
    "        print(\"Starting job extraction...\")\n",
    "        jobs_data = extract_jobs_data(API_KEY, num_pages=3)  # Explicitly requesting 3 pages\n",
    "        \n",
    "        if not jobs_data:\n",
    "            print(\"No jobs were found!\")\n",
    "            return\n",
    "            \n",
    "        # Save to JSON file\n",
    "        save_to_json(jobs_data)\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\nExtraction completed successfully!\")\n",
    "        print(f\"Total jobs extracted: {len(jobs_data)}\")\n",
    "        \n",
    "        # Print sample of first job and last job to verify pagination\n",
    "        if jobs_data:\n",
    "            print(\"\\nFirst job entry:\")\n",
    "            first_job = jobs_data[0]\n",
    "            for key, value in first_job.items():\n",
    "                if key == 'description':\n",
    "                    print(f\"{key}: {value[:150]}...\")\n",
    "                else:\n",
    "                    print(f\"{key}: {value}\")\n",
    "                    \n",
    "            if len(jobs_data) > 1:\n",
    "                print(\"\\nLast job entry:\")\n",
    "                last_job = jobs_data[-1]\n",
    "                for key, value in last_job.items():\n",
    "                    if key == 'description':\n",
    "                        print(f\"{key}: {value[:150]}...\")\n",
    "                    else:\n",
    "                        print(f\"{key}: {value}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9099b703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting job extraction...\n",
      "Found 10 jobs on page 1\n",
      "Successfully processed page 1 - Total jobs so far: 10\n",
      "Found 10 jobs on page 2\n",
      "Successfully processed page 2 - Total jobs so far: 20\n",
      "Found 10 jobs on page 3\n",
      "Successfully processed page 3 - Total jobs so far: 30\n",
      "\n",
      "Saved 30 jobs to software_engineer_jobs.json\n",
      "\n",
      "Successfully saved 30 jobs to software_engineer_jobs.csv\n",
      "\n",
      "Extraction completed successfully!\n",
      "Total jobs extracted: 30\n",
      "\n",
      "Sample of first job entry:\n",
      "title: Lead Software Engineer-Java, Bank Modernization\n",
      "company: Capital One\n",
      "location: New York, NY\n",
      "description: 114 5th Ave (22114), United States of America, New York, New York\n",
      "\n",
      "Lead Software Engineer-Java, Bank Modernization\n",
      "\n",
      "Do you love building and pioneerin...\n",
      "posted_at: 5 days ago\n",
      "apply_links: Capital One Careers: https://www.capitalonecareers.com/job/new-york/lead-software-engineer-java-bank-modernization/1732/73175694832?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic | Dice: https://www.dice.com/job-detail/27ea88d9-e140-4b37-beff-52925a4eca33?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic | ZipRecruiter: https://www.ziprecruiter.com/c/Capitalone/Job/Lead-Software-Engineer-Java,-Bank-Modernization/-in-New-York,NY?jid=6628534494d88ddc&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic | LinkedIn: https://www.linkedin.com/jobs/view/lead-software-engineer-java-bank-modernization-at-capital-one-4080357117?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic | SitePoint: https://www.sitepoint.com/jobs-for-developers/capital-one/lead-software-engineer-java-bank-modernization-941777/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic | SaluteMyJob: https://salutemyjob.com/jobs/lead-software-engineer-java-bank-modernization-new-york/1502637681-2/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic | Built In NYC: https://www.builtinnyc.com/job/lead-software-engineer-java-bank-modernization/293857?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic | IHire: https://www.ihire.com/jobs/view/454741032?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic\n"
     ]
    }
   ],
   "source": [
    "#saves as json and csv for all 3 pages jobs data\n",
    "from serpapi import GoogleSearch\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "def extract_jobs_data(api_key, num_pages=3):\n",
    "    all_jobs = []\n",
    "    current_page = 1\n",
    "    next_page_token = None\n",
    "    \n",
    "    while current_page <= num_pages:\n",
    "        # Parameters for the API request\n",
    "        params = {\n",
    "            'api_key': api_key,                  \n",
    "            'engine': 'google_jobs',             \n",
    "            'q': 'software engineer',            \n",
    "            'hl': 'en',                         \n",
    "            'gl': 'us',                         \n",
    "            'google_domain': 'google.com'        \n",
    "        }\n",
    "        \n",
    "        # Add next_page_token if we have one (for pages after first)\n",
    "        if next_page_token:\n",
    "            params['next_page_token'] = next_page_token\n",
    "        \n",
    "        try:\n",
    "            # Make the API request\n",
    "            search = GoogleSearch(params)\n",
    "            results = search.get_dict()\n",
    "            \n",
    "            # Check if we have job results and it's not empty\n",
    "            if 'jobs_results' not in results or not results['jobs_results']:\n",
    "                print(f\"No more results found on page {current_page}\")\n",
    "                break\n",
    "            \n",
    "            # Extract jobs from current page\n",
    "            jobs_on_this_page = results['jobs_results']\n",
    "            print(f\"Found {len(jobs_on_this_page)} jobs on page {current_page}\")\n",
    "            \n",
    "            # Extract specific parameters from each job\n",
    "            for job in jobs_on_this_page:\n",
    "                # Get all apply links if available\n",
    "                apply_links = []\n",
    "                if job.get('apply_options'):\n",
    "                    for option in job['apply_options']:\n",
    "                        if option.get('link'):\n",
    "                            apply_links.append(f\"{option.get('title', 'Unknown')}: {option.get('link')}\")\n",
    "                \n",
    "                job_data = {\n",
    "                    'title': job.get('title', 'N/A'),\n",
    "                    'company': job.get('company_name', 'N/A'),\n",
    "                    'location': job.get('location', 'N/A'),\n",
    "                    'description': job.get('description', 'N/A')[:500] + '...' if job.get('description') else 'N/A',\n",
    "                    'posted_at': job.get('detected_extensions', {}).get('posted_at', 'N/A'),\n",
    "                    'apply_links': ' | '.join(apply_links) if apply_links else 'N/A'\n",
    "                }\n",
    "                all_jobs.append(job_data)\n",
    "            \n",
    "            print(f\"Successfully processed page {current_page} - Total jobs so far: {len(all_jobs)}\")\n",
    "            \n",
    "            # Check for next page token\n",
    "            serpapi_pagination = results.get('serpapi_pagination', {})\n",
    "            next_page_token = serpapi_pagination.get('next_page_token')\n",
    "            \n",
    "            if not next_page_token:\n",
    "                print(f\"No next page token found after page {current_page}\")\n",
    "                break\n",
    "                \n",
    "            current_page += 1\n",
    "            time.sleep(2)  # Small delay between requests\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing page {current_page}: {str(e)}\")\n",
    "            break\n",
    "    \n",
    "    return all_jobs\n",
    "\n",
    "def save_to_csv(jobs_data, filename='software_engineer_jobs.csv'):\n",
    "    \"\"\"Save the extracted jobs data to a CSV file\"\"\"\n",
    "    # Define the fieldnames for the CSV\n",
    "    fieldnames = ['title', 'company', 'location', 'description', 'posted_at', 'apply_links']\n",
    "    \n",
    "    try:\n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            \n",
    "            # Write the header\n",
    "            writer.writeheader()\n",
    "            \n",
    "            # Write the job data\n",
    "            for job in jobs_data:\n",
    "                writer.writerow(job)\n",
    "                \n",
    "        print(f\"\\nSuccessfully saved {len(jobs_data)} jobs to {filename}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to CSV: {str(e)}\")\n",
    "\n",
    "def save_to_json(jobs_data, filename='software_engineer_jobs.json'):\n",
    "    \"\"\"Save the extracted jobs data to a JSON file\"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(jobs_data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"\\nSaved {len(jobs_data)} jobs to {filename}\")\n",
    "\n",
    "def main():\n",
    "    # Your SerpApi key\n",
    "    API_KEY = \"your key\"\n",
    "    \n",
    "    try:\n",
    "        # Extract jobs data\n",
    "        print(\"Starting job extraction...\")\n",
    "        jobs_data = extract_jobs_data(API_KEY, num_pages=3)\n",
    "        \n",
    "        if not jobs_data:\n",
    "            print(\"No jobs were found!\")\n",
    "            return\n",
    "            \n",
    "        # Save to both JSON and CSV\n",
    "        save_to_json(jobs_data)\n",
    "        save_to_csv(jobs_data)\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\nExtraction completed successfully!\")\n",
    "        print(f\"Total jobs extracted: {len(jobs_data)}\")\n",
    "        \n",
    "        # Print sample of first job\n",
    "        if jobs_data:\n",
    "            print(\"\\nSample of first job entry:\")\n",
    "            first_job = jobs_data[0]\n",
    "            for key, value in first_job.items():\n",
    "                if key == 'description':\n",
    "                    print(f\"{key}: {value[:150]}...\")\n",
    "                else:\n",
    "                    print(f\"{key}: {value}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b972788",
   "metadata": {},
   "outputs": [],
   "source": [
    "#multi-job data extraction while above is for single file\n",
    "from serpapi import GoogleSearch\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "def extract_jobs_for_title(api_key, job_title, num_pages=3):\n",
    "    all_jobs = []\n",
    "    current_page = 1\n",
    "    next_page_token = None\n",
    "    \n",
    "    while current_page <= num_pages:\n",
    "        # Parameters for the API request\n",
    "        params = {\n",
    "            'api_key': api_key,                  \n",
    "            'engine': 'google_jobs',             \n",
    "            'q': job_title,            \n",
    "            'hl': 'en',                         \n",
    "            'gl': 'us',                         \n",
    "            'google_domain': 'google.com'        \n",
    "        }\n",
    "        \n",
    "        # Add next_page_token if we have one (for pages after first)\n",
    "        if next_page_token:\n",
    "            params['next_page_token'] = next_page_token\n",
    "        \n",
    "        try:\n",
    "            # Make the API request\n",
    "            search = GoogleSearch(params)\n",
    "            results = search.get_dict()\n",
    "            \n",
    "            # Check if we have job results and it's not empty\n",
    "            if 'jobs_results' not in results or not results['jobs_results']:\n",
    "                print(f\"No more results found on page {current_page} for {job_title}\")\n",
    "                break\n",
    "            \n",
    "            # Extract jobs from current page\n",
    "            jobs_on_this_page = results['jobs_results']\n",
    "            print(f\"Found {len(jobs_on_this_page)} jobs on page {current_page} for {job_title}\")\n",
    "            \n",
    "            # Extract specific parameters from each job\n",
    "            for job in jobs_on_this_page:\n",
    "                # Get all apply links if available\n",
    "                apply_links = []\n",
    "                if job.get('apply_options'):\n",
    "                    for option in job['apply_options']:\n",
    "                        if option.get('link'):\n",
    "                            apply_links.append(f\"{option.get('title', 'Unknown')}: {option.get('link')}\")\n",
    "                \n",
    "                job_data = {\n",
    "                    'search_query': job_title,  # Add the search query that found this job\n",
    "                    'title': job.get('title', 'N/A'),\n",
    "                    'company': job.get('company_name', 'N/A'),\n",
    "                    'location': job.get('location', 'N/A'),\n",
    "                    'description': job.get('description', 'N/A')[:500] + '...' if job.get('description') else 'N/A',\n",
    "                    'posted_at': job.get('detected_extensions', {}).get('posted_at', 'N/A'),\n",
    "                    'apply_links': ' | '.join(apply_links) if apply_links else 'N/A'\n",
    "                }\n",
    "                all_jobs.append(job_data)\n",
    "            \n",
    "            print(f\"Successfully processed page {current_page} for {job_title} - Total jobs so far: {len(all_jobs)}\")\n",
    "            \n",
    "            # Check for next page token\n",
    "            serpapi_pagination = results.get('serpapi_pagination', {})\n",
    "            next_page_token = serpapi_pagination.get('next_page_token')\n",
    "            \n",
    "            if not next_page_token:\n",
    "                print(f\"No next page token found after page {current_page} for {job_title}\")\n",
    "                break\n",
    "                \n",
    "            current_page += 1\n",
    "            time.sleep(2)  # Small delay between requests\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing page {current_page} for {job_title}: {str(e)}\")\n",
    "            break\n",
    "    \n",
    "    return all_jobs\n",
    "\n",
    "def save_to_csv(jobs_data, filename='tech_jobs.csv'):\n",
    "    \"\"\"Save the extracted jobs data to a CSV file\"\"\"\n",
    "    # Define the fieldnames for the CSV\n",
    "    fieldnames = ['search_query', 'title', 'company', 'location', 'description', 'posted_at', 'apply_links']\n",
    "    \n",
    "    try:\n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            \n",
    "            # Write the header\n",
    "            writer.writeheader()\n",
    "            \n",
    "            # Write the job data\n",
    "            for job in jobs_data:\n",
    "                writer.writerow(job)\n",
    "                \n",
    "        print(f\"\\nSuccessfully saved {len(jobs_data)} jobs to {filename}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to CSV: {str(e)}\")\n",
    "\n",
    "def save_to_json(jobs_data, filename='tech_jobs.json'):\n",
    "    \"\"\"Save the extracted jobs data to a JSON file\"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(jobs_data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"\\nSaved {len(jobs_data)} jobs to {filename}\")\n",
    "\n",
    "def main():\n",
    "    # Your SerpApi key\n",
    "    API_KEY = \"your key\"\n",
    "    \n",
    "    # Define job titles to search for\n",
    "    job_titles = [\n",
    "        'software engineer',\n",
    "        'data engineer',\n",
    "        'data scientist'\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # List to store all jobs from all searches\n",
    "        all_jobs = []\n",
    "        \n",
    "        # Extract jobs data for each job title\n",
    "        print(\"Starting job extraction...\")\n",
    "        for job_title in job_titles:\n",
    "            print(f\"\\nSearching for {job_title} positions...\")\n",
    "            jobs = extract_jobs_for_title(API_KEY, job_title)\n",
    "            all_jobs.extend(jobs)\n",
    "            print(f\"Found {len(jobs)} {job_title} positions\")\n",
    "            time.sleep(3)  # Add delay between different job title searches\n",
    "        \n",
    "        if not all_jobs:\n",
    "            print(\"No jobs were found!\")\n",
    "            return\n",
    "            \n",
    "        # Save all jobs to both JSON and CSV\n",
    "        save_to_json(all_jobs)\n",
    "        save_to_csv(all_jobs)\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\nExtraction completed successfully!\")\n",
    "        print(f\"Total jobs extracted: {len(all_jobs)}\")\n",
    "        \n",
    "        # Print summary by job title\n",
    "        for job_title in job_titles:\n",
    "            count = len([job for job in all_jobs if job['search_query'] == job_title])\n",
    "            print(f\"- {job_title}: {count} positions\")\n",
    "        \n",
    "        # Print sample of first job for each type\n",
    "        for job_title in job_titles:\n",
    "            matching_jobs = [job for job in all_jobs if job['search_query'] == job_title]\n",
    "            if matching_jobs:\n",
    "                print(f\"\\nSample of first {job_title} job entry:\")\n",
    "                first_job = matching_jobs[0]\n",
    "                for key, value in first_job.items():\n",
    "                    if key == 'description':\n",
    "                        print(f\"{key}: {value[:150]}...\")\n",
    "                    else:\n",
    "                        print(f\"{key}: {value}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0bdec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting job extraction...\n",
      "\n",
      "Searching for software engineer positions...\n",
      "Found 10 jobs on page 1 for software engineer\n",
      "Successfully processed page 1 for software engineer - Total jobs so far: 10\n",
      "Found 10 jobs on page 2 for software engineer\n",
      "Successfully processed page 2 for software engineer - Total jobs so far: 20\n",
      "Found 10 jobs on page 3 for software engineer\n",
      "Successfully processed page 3 for software engineer - Total jobs so far: 30\n",
      "Found 30 software engineer positions\n",
      "\n",
      "Searching for data engineer positions...\n",
      "Found 10 jobs on page 1 for data engineer\n",
      "Successfully processed page 1 for data engineer - Total jobs so far: 10\n",
      "Found 9 jobs on page 2 for data engineer\n",
      "Successfully processed page 2 for data engineer - Total jobs so far: 19\n",
      "Found 8 jobs on page 3 for data engineer\n",
      "Successfully processed page 3 for data engineer - Total jobs so far: 27\n",
      "Found 27 data engineer positions\n",
      "\n",
      "Searching for data scientist positions...\n",
      "Found 10 jobs on page 1 for data scientist\n",
      "Successfully processed page 1 for data scientist - Total jobs so far: 10\n",
      "Found 10 jobs on page 2 for data scientist\n",
      "Successfully processed page 2 for data scientist - Total jobs so far: 20\n",
      "Found 10 jobs on page 3 for data scientist\n",
      "Successfully processed page 3 for data scientist - Total jobs so far: 30\n",
      "Found 30 data scientist positions\n",
      "\n",
      "Saved 87 jobs to tech_jobs.json\n",
      "\n",
      "Successfully saved 87 jobs to tech_jobs.csv\n",
      "\n",
      "Extraction completed successfully!\n",
      "Total jobs extracted: 87\n",
      "- software engineer: 30 positions\n",
      "- data engineer: 27 positions\n",
      "- data scientist: 30 positions\n",
      "\n",
      "Sample job entry with actual posted date:\n",
      "search_query: software engineer\n",
      "title: Lead Software Engineer-Java, Bank Modernization\n",
      "company: Capital One\n",
      "location: New York, NY\n",
      "description: 114 5th Ave (22114), United States of America, New York, New York\n",
      "\n",
      "Lead Software Engineer-Java, Bank Modernization\n",
      "\n",
      "Do you love building and pioneerin...\n",
      "posted_at: 5 days ago\n",
      "posted_date: 2024-11-17\n",
      "apply_links: Capital One Careers: https://www.capitalonecareers.com/job/new-york/lead-software-engineer-java-bank-modernization/1732/73175694832?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic | Dice: https://www.dice.com/job-detail/27ea88d9-e140-4b37-beff-52925a4eca33?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic | ZipRecruiter: https://www.ziprecruiter.com/c/Capitalone/Job/Lead-Software-Engineer-Java,-Bank-Modernization/-in-New-York,NY?jid=6628534494d88ddc&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic | LinkedIn: https://www.linkedin.com/jobs/view/lead-software-engineer-java-bank-modernization-at-capital-one-4080357117?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic | SitePoint: https://www.sitepoint.com/jobs-for-developers/capital-one/lead-software-engineer-java-bank-modernization-941777/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic | SaluteMyJob: https://salutemyjob.com/jobs/lead-software-engineer-java-bank-modernization-new-york/1502637681-2/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic | Built In NYC: https://www.builtinnyc.com/job/lead-software-engineer-java-bank-modernization/293857?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic | IHire: https://www.ihire.com/jobs/view/454741032?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic\n"
     ]
    }
   ],
   "source": [
    "#multi-page, multi-jobs, with proper dates for posted job added in CSV\n",
    "from serpapi import GoogleSearch\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import re\n",
    "\n",
    "def extract_days_ago(posted_at):\n",
    "    \"\"\"Extract number of days from posted_at string\"\"\"\n",
    "    if not posted_at or posted_at == 'N/A':\n",
    "        return None\n",
    "        \n",
    "    # Extract numbers using regex\n",
    "    match = re.search(r'(\\d+)', posted_at)\n",
    "    if match:\n",
    "        days = int(match.group(1))\n",
    "        if 'month' in posted_at.lower():\n",
    "            days = days * 30  # Approximate month to days\n",
    "        elif 'week' in posted_at.lower():\n",
    "            days = days * 7   # Convert weeks to days\n",
    "        return days\n",
    "    elif 'hour' in posted_at.lower() or 'today' in posted_at.lower():\n",
    "        return 0\n",
    "    return None\n",
    "\n",
    "def extract_time_info(posted_at):\n",
    "    \"\"\"\n",
    "    Extract time information from posted_at string\n",
    "    Returns tuple of (number, unit)\n",
    "    \"\"\"\n",
    "    if not posted_at or posted_at == 'N/A':\n",
    "        return None, None\n",
    "    \n",
    "    # Extract numbers using regex\n",
    "    match = re.search(r'(\\d+)\\s*(\\w+)', posted_at.lower())\n",
    "    if not match:\n",
    "        return None, None\n",
    "        \n",
    "    number = int(match.group(1))\n",
    "    unit = match.group(2).rstrip('s')  # remove plural 's' if present\n",
    "    \n",
    "    return number, unit\n",
    "\n",
    "def calculate_posted_date(posted_at):\n",
    "    \"\"\"\n",
    "    Calculate actual posted date from time ago format\n",
    "    Keeps same date for hours, changes date for days/weeks/months\n",
    "    \"\"\"\n",
    "    if not posted_at or posted_at == 'N/A':\n",
    "        return 'N/A'\n",
    "    \n",
    "    number, unit = extract_time_info(posted_at)\n",
    "    if not number or not unit:\n",
    "        return 'N/A'\n",
    "    \n",
    "    today = datetime.now()\n",
    "    \n",
    "    # Handle different time units\n",
    "    if 'hour' in unit:\n",
    "        # For hours, keep today's date\n",
    "        return today.strftime('%Y-%m-%d')\n",
    "    elif 'day' in unit:\n",
    "        posted_date = today - timedelta(days=number)\n",
    "    elif 'week' in unit:\n",
    "        posted_date = today - timedelta(days=number * 7)\n",
    "    elif 'month' in unit:\n",
    "        posted_date = today - timedelta(days=number * 30)  # approximate\n",
    "    else:\n",
    "        return 'N/A'\n",
    "    \n",
    "    return posted_date.strftime('%Y-%m-%d')\n",
    "\n",
    "def extract_jobs_for_title(api_key, job_title, num_pages=3):\n",
    "    all_jobs = []\n",
    "    current_page = 1\n",
    "    next_page_token = None\n",
    "    \n",
    "    while current_page <= num_pages:\n",
    "        params = {\n",
    "            'api_key': api_key,                  \n",
    "            'engine': 'google_jobs',             \n",
    "            'q': job_title,            \n",
    "            'hl': 'en',                         \n",
    "            'gl': 'us',                         \n",
    "            'google_domain': 'google.com'        \n",
    "        }\n",
    "        \n",
    "        if next_page_token:\n",
    "            params['next_page_token'] = next_page_token\n",
    "        \n",
    "        try:\n",
    "            search = GoogleSearch(params)\n",
    "            results = search.get_dict()\n",
    "            \n",
    "            if 'jobs_results' not in results or not results['jobs_results']:\n",
    "                print(f\"No more results found on page {current_page} for {job_title}\")\n",
    "                break\n",
    "            \n",
    "            jobs_on_this_page = results['jobs_results']\n",
    "            print(f\"Found {len(jobs_on_this_page)} jobs on page {current_page} for {job_title}\")\n",
    "            \n",
    "            for job in jobs_on_this_page:\n",
    "                apply_links = []\n",
    "                if job.get('apply_options'):\n",
    "                    for option in job['apply_options']:\n",
    "                        if option.get('link'):\n",
    "                            apply_links.append(f\"{option.get('title', 'Unknown')}: {option.get('link')}\")\n",
    "                \n",
    "                posted_at = job.get('detected_extensions', {}).get('posted_at', 'N/A')\n",
    "                posted_date = calculate_posted_date(posted_at)\n",
    "                \n",
    "                job_data = {\n",
    "                    'search_query': job_title,\n",
    "                    'title': job.get('title', 'N/A'),\n",
    "                    'company': job.get('company_name', 'N/A'),\n",
    "                    'location': job.get('location', 'N/A'),\n",
    "                    'description': job.get('description', 'N/A')[:500] + '...' if job.get('description') else 'N/A',\n",
    "                    'posted_at': posted_at,\n",
    "                    'posted_date': posted_date,\n",
    "                    'apply_links': ' | '.join(apply_links) if apply_links else 'N/A'\n",
    "                }\n",
    "                all_jobs.append(job_data)\n",
    "            \n",
    "            print(f\"Successfully processed page {current_page} for {job_title} - Total jobs so far: {len(all_jobs)}\")\n",
    "            \n",
    "            serpapi_pagination = results.get('serpapi_pagination', {})\n",
    "            next_page_token = serpapi_pagination.get('next_page_token')\n",
    "            \n",
    "            if not next_page_token:\n",
    "                print(f\"No next page token found after page {current_page} for {job_title}\")\n",
    "                break\n",
    "                \n",
    "            current_page += 1\n",
    "            time.sleep(2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing page {current_page} for {job_title}: {str(e)}\")\n",
    "            break\n",
    "    \n",
    "    return all_jobs\n",
    "\n",
    "def save_to_csv(jobs_data, filename='tech_jobs.csv'):\n",
    "    \"\"\"Save the extracted jobs data to a CSV file\"\"\"\n",
    "    fieldnames = ['search_query', 'title', 'company', 'location', 'description', 'posted_at', 'posted_date', 'apply_links']\n",
    "    \n",
    "    try:\n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(jobs_data)\n",
    "        print(f\"\\nSuccessfully saved {len(jobs_data)} jobs to {filename}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to CSV: {str(e)}\")\n",
    "\n",
    "def save_to_json(jobs_data, filename='tech_jobs.json'):\n",
    "    \"\"\"Save the extracted jobs data to a JSON file\"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(jobs_data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"\\nSaved {len(jobs_data)} jobs to {filename}\")\n",
    "\n",
    "def main():\n",
    "    API_KEY = \"your key\"\n",
    "    \n",
    "    job_titles = [\n",
    "        'software engineer',\n",
    "        'data engineer',\n",
    "        'data scientist'\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        all_jobs = []\n",
    "        \n",
    "        print(\"Starting job extraction...\")\n",
    "        for job_title in job_titles:\n",
    "            print(f\"\\nSearching for {job_title} positions...\")\n",
    "            jobs = extract_jobs_for_title(API_KEY, job_title)\n",
    "            all_jobs.extend(jobs)\n",
    "            print(f\"Found {len(jobs)} {job_title} positions\")\n",
    "            time.sleep(3)\n",
    "        \n",
    "        if not all_jobs:\n",
    "            print(\"No jobs were found!\")\n",
    "            return\n",
    "            \n",
    "        save_to_json(all_jobs)\n",
    "        save_to_csv(all_jobs)\n",
    "        \n",
    "        print(f\"\\nExtraction completed successfully!\")\n",
    "        print(f\"Total jobs extracted: {len(all_jobs)}\")\n",
    "        \n",
    "        # Print summary by job title\n",
    "        for job_title in job_titles:\n",
    "            count = len([job for job in all_jobs if job['search_query'] == job_title])\n",
    "            print(f\"- {job_title}: {count} positions\")\n",
    "        \n",
    "        # Print sample entry\n",
    "        if all_jobs:\n",
    "            print(\"\\nSample job entry with actual posted date:\")\n",
    "            first_job = all_jobs[0]\n",
    "            for key, value in first_job.items():\n",
    "                if key == 'description':\n",
    "                    print(f\"{key}: {value[:150]}...\")\n",
    "                else:\n",
    "                    print(f\"{key}: {value}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7776143",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
